import os
import re
import pandas as pd
import pytz
import shutil
import zipfile
import requests
import logging
from datetime import datetime, time
from sqlalchemy import create_engine
from concurrent.futures import ThreadPoolExecutor
from config import DATA_DIR, DATABASE_URL
import cloudinary
import cloudinary.uploader
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s: %(message)s")

MAX_TELEGRAM_FILE_MB = 50
BEIJING_TZ = pytz.timezone("Asia/Shanghai")

# ç­æ¬¡æ—¶é—´å®šä¹‰
SHIFT_TIMES = {
    "Fç­": (time(12, 0), time(21, 0)),
    "Gç­": (time(13, 0), time(22, 0)),
    "Hç­": (time(14, 0), time(23, 0)),
    "Iç­": (time(15, 0), time(0, 0)),  # è·¨å¤©å¤„ç†
}

def safe_filename(name: str) -> str:
    return re.sub(r'[\\/*?:"<>|]', "_", str(name))

def upload_to_cloudinary(file_path: str) -> str | None:
    try:
        result = cloudinary.uploader.upload(
            file_path,
            resource_type="raw",
            folder="telegram_exports",
            public_id=os.path.splitext(os.path.basename(file_path))[0]
        )
        return result.get("secure_url")
    except Exception as e:
        logging.error(f"âŒ Cloudinary ä¸Šä¼ å¤±è´¥: {e}")
        return None

def _fetch_data(start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:
    try:
        engine = create_engine(DATABASE_URL)
        query = """
        SELECT username, name, content, timestamp, keyword, shift 
        FROM messages 
        WHERE timestamp BETWEEN %(start)s AND %(end)s
        """
        params = {
            "start": start_datetime.astimezone(pytz.UTC),
            "end": end_datetime.astimezone(pytz.UTC)
        }
        df_iter = pd.read_sql_query(query, engine, params=params, chunksize=50000)
        df = pd.concat(df_iter, ignore_index=True)
        logging.info(f"âœ… æ•°æ®è¯»å–å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
    except Exception as e:
        logging.error(f"âŒ æ— æ³•è¿æ¥æ•°æ®åº“æˆ–è¯»å–æ•°æ®: {e}")
        return pd.DataFrame()

    if df.empty:
        return df

    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True).dt.tz_convert(BEIJING_TZ)
    df = df.dropna(subset=["timestamp"]).copy()
    return df

def _mark_late_early(excel_path: str):
    """æ ‡æ³¨è¿Ÿåˆ°ã€æ—©é€€ï¼ˆçº¢è‰²+ç­æ¬¡æ ‡è¯†ï¼‰å’Œè¡¥å¡ï¼ˆé»„è‰²+ç­æ¬¡æ ‡è¯†ï¼‰ï¼Œä¸‹ç­è¶…è¿‡æ—¶é—´æ­£å¸¸"""
    wb = load_workbook(excel_path)
    fill_red = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
    fill_yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    for sheet in wb.worksheets:
        for row in sheet.iter_rows(min_row=2):
            shift_cell, time_cell, keyword_cell = row[3], row[1], row[2]
            if not shift_cell.value or not time_cell.value:
                continue

            shift_text = str(shift_cell.value)
            shift_name = re.split(r'[ï¼ˆ(]', shift_text)[0]

            # è¡¥å¡æ ‡è®°
            if "è¡¥å¡" in shift_text:
                time_cell.fill = fill_yellow
                shift_cell.fill = fill_yellow
                if "ï¼ˆè¡¥å¡ï¼‰" not in shift_text:
                    shift_cell.value = f"{shift_text}ï¼ˆè¡¥å¡ï¼‰"
                continue

            # è¿Ÿåˆ°/æ—©é€€åˆ¤å®š
            if shift_name in SHIFT_TIMES:
                start_time, end_time = SHIFT_TIMES[shift_name]
                dt = datetime.strptime(time_cell.value, "%Y-%m-%d %H:%M:%S")

                # è¿Ÿåˆ°ï¼šä¸Šç­æ‰“å¡ > å¼€å§‹æ—¶é—´
                if keyword_cell.value == "#ä¸Šç­æ‰“å¡":
                    if dt.time() > start_time:
                        time_cell.fill = fill_red
                        shift_cell.fill = fill_red
                        if "ï¼ˆè¿Ÿåˆ°ï¼‰" not in shift_text:
                            shift_cell.value = f"{shift_text}ï¼ˆè¿Ÿåˆ°ï¼‰"

                # æ—©é€€ï¼šä¸‹ç­æ‰“å¡ < ç»“æŸæ—¶é—´ï¼ˆè¶…è¿‡æ—¶é—´æ­£å¸¸ï¼Œä¸æç¤ºåŠ ç­ï¼‰
                elif keyword_cell.value == "#ä¸‹ç­æ‰“å¡":
                    if shift_name == "Iç­" and dt.hour == 0:
                        continue  # Iç­å‡Œæ™¨ä¸‹ç­æ­£å¸¸
                    if dt.time() < end_time:
                        time_cell.fill = fill_red
                        shift_cell.fill = fill_red
                        if "ï¼ˆæ—©é€€ï¼‰" not in shift_text:
                            shift_cell.value = f"{shift_text}ï¼ˆæ—©é€€ï¼‰"

    wb.save(excel_path)


def export_excel(start_datetime: datetime, end_datetime: datetime):
    df = _fetch_data(start_datetime, end_datetime)
    if df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰æ•°æ®")
        return None

    # æ·»åŠ æ—¥æœŸåˆ—
    df["date"] = df["timestamp"].dt.strftime("%Y-%m-%d")

    start_str = start_datetime.strftime("%Y-%m-%d")
    end_str = (end_datetime - pd.Timedelta(seconds=1)).strftime("%Y-%m-%d")

    export_dir = os.path.join(DATA_DIR, f"excel_{start_str}_{end_str}")
    os.makedirs(export_dir, exist_ok=True)
    excel_path = os.path.join(export_dir, f"æ‰“å¡è®°å½•_{start_str}_{end_str}.xlsx")

    # æ ¼å¼åŒ–ç­æ¬¡å‡½æ•°ï¼Œé¿å…é‡å¤æ·»åŠ æ—¶é—´æ®µ
    def format_shift(shift):
        if pd.isna(shift):
            return shift
        shift_text = str(shift)

        # å¦‚æœå·²å­˜åœ¨ "ï¼ˆHH:MM-HH:MMï¼‰" æ ¼å¼ï¼Œç›´æ¥è¿”å›
        if re.search(r'ï¼ˆ\d{2}:\d{2}-\d{2}:\d{2}ï¼‰', shift_text):
            return shift_text

        shift_name = shift_text.split("ï¼ˆ")[0]  # å»æ‰â€œè¡¥å¡â€æ ‡è®°ç­‰
        if shift_name in SHIFT_TIMES:
            start, end = SHIFT_TIMES[shift_name]
            end_str = end.strftime('%H:%M')  # Iç­ä¹Ÿæ˜¾ç¤º00:00ï¼Œä¸åŠ â€œæ¬¡æ—¥â€
            return f"{shift_text}ï¼ˆ{start.strftime('%H:%M')}-{end_str}ï¼‰"
        return shift_text

    # æŒ‰æ—¥æœŸåˆ†è¡¨å†™å…¥ Excel
    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        for day, group_df in df.groupby("date"):
            slim_df = group_df[["name", "timestamp", "keyword", "shift"]].sort_values("timestamp").copy()
            slim_df.columns = ["å§“å", "æ‰“å¡æ—¶é—´", "å…³é”®è¯", "ç­æ¬¡"]
            slim_df["æ‰“å¡æ—¶é—´"] = slim_df["æ‰“å¡æ—¶é—´"].dt.strftime("%Y-%m-%d %H:%M:%S")

            # æ ¼å¼åŒ–ç­æ¬¡åˆ—ï¼ˆå¦‚ Iç­ â†’ Iç­ï¼ˆ15:00-00:00ï¼‰ï¼‰
            slim_df["ç­æ¬¡"] = slim_df["ç­æ¬¡"].apply(format_shift)

            slim_df.to_excel(writer, sheet_name=day[:31], index=False)

    # æ ‡æ³¨è¿Ÿåˆ°/æ—©é€€å’Œè¡¥å¡
    _mark_late_early(excel_path)
    logging.info(f"âœ… Excel å¯¼å‡ºå®Œæˆå¹¶æ ‡æ³¨è¿Ÿåˆ°/æ—©é€€: {excel_path}")
    return excel_path

def export_images(start_datetime: datetime, end_datetime: datetime):
    df = _fetch_data(start_datetime, end_datetime)
    if df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰æ•°æ®")
        return None

    photo_df = df[df["content"].str.contains(r"\.jpg|\.jpeg|\.png", case=False, na=False)]
    if photo_df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰å›¾ç‰‡")
        return None

    start_str = start_datetime.strftime("%Y-%m-%d")
    end_str = (end_datetime - pd.Timedelta(seconds=1)).strftime("%Y-%m-%d")
    export_dir = os.path.join(DATA_DIR, f"images_{start_str}_{end_str}")
    os.makedirs(export_dir, exist_ok=True)

    def download_image(row):
        url = row["content"]
        if url and url.startswith("http"):
            try:
                ts = row["timestamp"].strftime("%Y-%m-%d_%H-%M-%S")
                date_folder = row["timestamp"].strftime("%Y-%m-%d")
                day_dir = os.path.join(export_dir, date_folder)
                os.makedirs(day_dir, exist_ok=True)

                name = safe_filename(row["name"] or "åŒ¿å")
                keyword = safe_filename(row["keyword"] or "æ— å…³é”®è¯")
                filename = f"{ts}_{name}_{keyword}.jpg"
                save_path = os.path.join(day_dir, filename)

                response = requests.get(url, stream=True, timeout=10)
                if response.status_code == 200:
                    with open(save_path, "wb") as f:
                        for chunk in response.iter_content(1024):
                            f.write(chunk)
                logging.info(f"ğŸ“¥ ä¸‹è½½æˆåŠŸ: {filename}")
            except Exception as e:
                logging.warning(f"[å›¾ç‰‡ä¸‹è½½å¤±è´¥] {url} - {e}")

    with ThreadPoolExecutor(max_workers=8) as executor:
        executor.map(download_image, photo_df.to_dict("records"))

    zip_path = os.path.join(DATA_DIR, f"å›¾ç‰‡æ‰“åŒ…_{start_str}_{end_str}.zip")
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(export_dir):
            for file in files:
                full_path = os.path.join(root, file)
                arcname = os.path.relpath(full_path, export_dir)
                zipf.write(full_path, arcname)

    shutil.rmtree(export_dir)
    logging.info(f"âœ… å›¾ç‰‡æ‰“åŒ…å®Œæˆ: {zip_path}")

    file_size_mb = os.path.getsize(zip_path) / (1024 * 1024)
    if file_size_mb > MAX_TELEGRAM_FILE_MB:
        logging.warning(f"âš ï¸ æ–‡ä»¶è¶…è¿‡ {MAX_TELEGRAM_FILE_MB}MBï¼Œå°è¯•ä¸Šä¼ åˆ° Cloudinary...")
        url = upload_to_cloudinary(zip_path)
        if url:
            os.remove(zip_path)
            return url
        else:
            return None

    return zip_path
