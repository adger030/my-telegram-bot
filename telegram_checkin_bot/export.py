import os
import re
import pandas as pd
import pytz
import shutil
import zipfile
import requests
import logging
from datetime import datetime
from sqlalchemy import create_engine
from concurrent.futures import ThreadPoolExecutor
from config import DATA_DIR, DATABASE_URL
import cloudinary
import cloudinary.uploader

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s: %(message)s")

MAX_TELEGRAM_FILE_MB = 50  # Telegram æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼‰
BEIJING_TZ = pytz.timezone("Asia/Shanghai")


def safe_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return re.sub(r'[\\/*?:"<>|]', "_", str(name))


def upload_to_cloudinary(file_path: str) -> str | None:
    """ä¸Šä¼ æ–‡ä»¶åˆ° Cloudinary å¹¶è¿”å›ä¸‹è½½é“¾æ¥"""
    try:
        result = cloudinary.uploader.upload(
            file_path,
            resource_type="raw",
            folder="telegram_exports",
            public_id=os.path.splitext(os.path.basename(file_path))[0]  # ç”¨æ–‡ä»¶åä½œä¸º ID
        )
        secure_url = result.get("secure_url")
        if secure_url:
            logging.info(f"âœ… Cloudinary ä¸Šä¼ æˆåŠŸ: {secure_url}")
            return secure_url
        else:
            logging.error("âŒ Cloudinary ä¸Šä¼ æœªè¿”å› secure_url")
            return None
    except Exception as e:
        logging.error(f"âŒ Cloudinary ä¸Šä¼ å¤±è´¥: {e}")
        return None


def _fetch_data(start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:
    """ä»æ•°æ®åº“è¯»å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®"""
    try:
        engine = create_engine(DATABASE_URL)
        query = """
        SELECT username, name, content, timestamp, keyword, shift 
        FROM messages 
        WHERE timestamp BETWEEN :start AND :end
        """
        df_iter = pd.read_sql_query(query, engine, params={"start": start_datetime, "end": end_datetime}, chunksize=50000)
        df = pd.concat(df_iter, ignore_index=True)
        logging.info(f"âœ… æ•°æ®è¯»å–å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
    except Exception as e:
        logging.error(f"âŒ æ— æ³•è¿æ¥æ•°æ®åº“æˆ–è¯»å–æ•°æ®: {e}")
        return pd.DataFrame()

    if df.empty:
        return df

    # è½¬æ¢æ—¶é—´ä¸ºåŒ—äº¬æ—¶é—´
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True).dt.tz_convert(BEIJING_TZ)
    df = df.dropna(subset=["timestamp"]).copy()
    return df


def export_excel(start_datetime: datetime, end_datetime: datetime):
    """ä»…å¯¼å‡º Excelï¼Œä¸åŒ…å«å›¾ç‰‡"""
    df = _fetch_data(start_datetime, end_datetime)
    if df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰æ•°æ®")
        return None

    df["date"] = df["timestamp"].dt.strftime("%Y-%m-%d")

    start_str = start_datetime.strftime("%Y-%m-%d")
    end_str = (end_datetime - pd.Timedelta(seconds=1)).strftime("%Y-%m-%d")
    export_dir = os.path.join(DATA_DIR, f"excel_{start_str}_{end_str}")
    os.makedirs(export_dir, exist_ok=True)

    excel_path = os.path.join(export_dir, f"æ‰“å¡è®°å½•_{start_str}_{end_str}.xlsx")
    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        for day, group_df in df.groupby("date"):
            slim_df = group_df[["name", "timestamp", "keyword", "shift"]].sort_values("timestamp").copy()
            slim_df.columns = ["å§“å", "æ‰“å¡æ—¶é—´", "å…³é”®è¯", "ç­æ¬¡"]
            slim_df["æ‰“å¡æ—¶é—´"] = slim_df["æ‰“å¡æ—¶é—´"].dt.strftime("%Y-%m-%d %H:%M:%S")
            slim_df.to_excel(writer, sheet_name=day[:31], index=False)

    logging.info(f"âœ… Excel å¯¼å‡ºå®Œæˆ: {excel_path}")
    return excel_path


def export_images(start_datetime: datetime, end_datetime: datetime):
    """ä»…å¯¼å‡ºå›¾ç‰‡å¹¶æ‰“åŒ… ZIP"""
    df = _fetch_data(start_datetime, end_datetime)
    if df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰æ•°æ®")
        return None

    photo_df = df[df["content"].str.endswith(".jpg", na=False)]
    if photo_df.empty:
        logging.warning("âš ï¸ æŒ‡å®šæ—¥æœŸå†…æ²¡æœ‰å›¾ç‰‡")
        return None

    start_str = start_datetime.strftime("%Y-%m-%d")
    end_str = (end_datetime - pd.Timedelta(seconds=1)).strftime("%Y-%m-%d")
    export_dir = os.path.join(DATA_DIR, f"images_{start_str}_{end_str}")
    os.makedirs(export_dir, exist_ok=True)

    def download_image(row):
        url = row["content"]
        if url and url.startswith("http"):
            try:
                ts = row["timestamp"].strftime("%Y-%m-%d_%H-%M-%S")
                date_folder = row["timestamp"].strftime("%Y-%m-%d")
                day_dir = os.path.join(export_dir, date_folder)
                os.makedirs(day_dir, exist_ok=True)

                name = safe_filename(row["name"] or "åŒ¿å")
                keyword = safe_filename(row["keyword"] or "æ— å…³é”®è¯")
                filename = f"{ts}_{name}_{keyword}.jpg"
                save_path = os.path.join(day_dir, filename)

                response = requests.get(url, stream=True, timeout=10)
                if response.status_code == 200:
                    with open(save_path, "wb") as f:
                        for chunk in response.iter_content(1024):
                            f.write(chunk)
                logging.info(f"ğŸ“¥ ä¸‹è½½æˆåŠŸ: {filename}")
            except Exception as e:
                logging.warning(f"[å›¾ç‰‡ä¸‹è½½å¤±è´¥] {url} - {e}")

    with ThreadPoolExecutor(max_workers=8) as executor:
        executor.map(download_image, photo_df.to_dict("records"))

    zip_path = os.path.join(DATA_DIR, f"å›¾ç‰‡æ‰“åŒ…_{start_str}_{end_str}.zip")
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(export_dir):
            for file in files:
                full_path = os.path.join(root, file)
                arcname = os.path.relpath(full_path, export_dir)
                zipf.write(full_path, arcname)

    shutil.rmtree(export_dir)
    logging.info(f"âœ… å›¾ç‰‡æ‰“åŒ…å®Œæˆ: {zip_path}")

    # æ£€æŸ¥å¤§å°å¹¶å†³å®šæ˜¯å¦ä¸Šä¼ åˆ° Cloudinary
    file_size_mb = os.path.getsize(zip_path) / (1024 * 1024)
    if file_size_mb > MAX_TELEGRAM_FILE_MB:
        logging.warning(f"âš ï¸ æ–‡ä»¶è¶…è¿‡ {MAX_TELEGRAM_FILE_MB}MBï¼Œå°è¯•ä¸Šä¼ åˆ° Cloudinary...")
        url = upload_to_cloudinary(zip_path)
        if url:
            os.remove(zip_path)
            return url
        else:
            return None

    return zip_path
